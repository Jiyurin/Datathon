{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749f5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 1: Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad8f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found dataset file: ctg_clean.xlsx\n",
      "Available sheets: ['ctg_clean']\n",
      " Loaded sheet: ctg_clean\n",
      "Shape: (2128, 1)\n",
      "Columns: ['FileName,Date,SegFile,b,e,LBE,LB,AC,FM,UC,ASTV,MSTV,ALTV,MLTV,DL,DS,DP,DR,Width,Min,Max,Nmax,Nzeros,Mode,Mean,Median,Variance,Tendency,NSP']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName,Date,SegFile,b,e,LBE,LB,AC,FM,UC,ASTV,MSTV,ALTV,MLTV,DL,DS,DP,DR,Width,Min,Max,Nmax,Nzeros,Mode,Mean,Median,Variance,Tendency,NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Variab10.txt,1996-12-01,CTG0001.txt,240.0,357....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0002.txt,5.0,632.0,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0003.txt,177.0,779.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0004.txt,411.0,1192.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0005.txt,533.0,1147.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FileName,Date,SegFile,b,e,LBE,LB,AC,FM,UC,ASTV,MSTV,ALTV,MLTV,DL,DS,DP,DR,Width,Min,Max,Nmax,Nzeros,Mode,Mean,Median,Variance,Tendency,NSP\n",
       "0  Variab10.txt,1996-12-01,CTG0001.txt,240.0,357....                                                                                        \n",
       "1  Fmcs_1.txt,1996-05-03,CTG0002.txt,5.0,632.0,13...                                                                                        \n",
       "2  Fmcs_1.txt,1996-05-03,CTG0003.txt,177.0,779.0,...                                                                                        \n",
       "3  Fmcs_1.txt,1996-05-03,CTG0004.txt,411.0,1192.0...                                                                                        \n",
       "4  Fmcs_1.txt,1996-05-03,CTG0005.txt,533.0,1147.0...                                                                                        "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell 2: Load CTG dataset \n",
    "files = os.listdir()\n",
    "candidates = [f for f in files if f.lower().startswith(\"ctg\") and f.lower().endswith((\".xls\", \".xlsx\", \".csv\"))]\n",
    "\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(\"No CTG dataset found. Place CTG.xls / CTG.xlsx / CTG.csv in this folder.\")\n",
    "\n",
    "file_name = candidates[0]\n",
    "print(\" Found dataset file:\", file_name)\n",
    "\n",
    "if file_name.lower().endswith(\".csv\"):\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(\" Loaded CSV file.\")\n",
    "else:\n",
    "    xls = pd.ExcelFile(file_name)\n",
    "    print(\"Available sheets:\", xls.sheet_names)\n",
    "    sheet_name = \"Raw Data\" if \"Raw Data\" in xls.sheet_names else (\n",
    "        \"Data\" if \"Data\" in xls.sheet_names else xls.sheet_names[0]\n",
    "    )\n",
    "    df = pd.read_excel(file_name, sheet_name=sheet_name)\n",
    "    print(\" Loaded sheet:\", sheet_name)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns[:10].tolist())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2aeff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after tidying: (2128, 1)\n",
      "Columns: ['FileName,Date,SegFile,b,e,LBE,LB,AC,FM,UC,ASTV,MSTV,ALTV,MLTV,DL,DS,DP,DR,Width,Min,Max,Nmax,Nzeros,Mode,Mean,Median,Variance,Tendency,NSP']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName,Date,SegFile,b,e,LBE,LB,AC,FM,UC,ASTV,MSTV,ALTV,MLTV,DL,DS,DP,DR,Width,Min,Max,Nmax,Nzeros,Mode,Mean,Median,Variance,Tendency,NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Variab10.txt,1996-12-01,CTG0001.txt,240.0,357....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0002.txt,5.0,632.0,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0003.txt,177.0,779.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0004.txt,411.0,1192.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0005.txt,533.0,1147.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FileName,Date,SegFile,b,e,LBE,LB,AC,FM,UC,ASTV,MSTV,ALTV,MLTV,DL,DS,DP,DR,Width,Min,Max,Nmax,Nzeros,Mode,Mean,Median,Variance,Tendency,NSP\n",
       "0  Variab10.txt,1996-12-01,CTG0001.txt,240.0,357....                                                                                        \n",
       "1  Fmcs_1.txt,1996-05-03,CTG0002.txt,5.0,632.0,13...                                                                                        \n",
       "2  Fmcs_1.txt,1996-05-03,CTG0003.txt,177.0,779.0,...                                                                                        \n",
       "3  Fmcs_1.txt,1996-05-03,CTG0004.txt,411.0,1192.0...                                                                                        \n",
       "4  Fmcs_1.txt,1996-05-03,CTG0005.txt,533.0,1147.0...                                                                                        "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Tidy the dataset\n",
    "def tidy_sheet(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cleaned = df.copy()\n",
    "    cleaned.columns = [str(col).strip() for col in cleaned.columns]\n",
    "    cleaned = cleaned.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "    cleaned = cleaned.loc[:, ~cleaned.columns.str.contains(\"^Unnamed\", case=False)]\n",
    "    cleaned = cleaned.loc[:, ~cleaned.columns.duplicated()]\n",
    "    return cleaned\n",
    "\n",
    "sheet2 = tidy_sheet(df)\n",
    "print(\"Shape after tidying:\", sheet2.shape)\n",
    "print(\"Columns:\", sheet2.columns.tolist()[:20])\n",
    "sheet2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8de71c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing Value Summary ===\n",
      "✅ No missing values detected.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(missing_summary[missing_summary \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m missing_summary\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ No missing values detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Demo: introduce NaNs into ASTV (or first numeric column), then impute\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m demo_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mASTV\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mASTV\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sheet2\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m sheet2\u001b[38;5;241m.\u001b[39mselect_dtypes(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m demo_df \u001b[38;5;241m=\u001b[39m sheet2[[demo_feature]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      9\u001b[0m rng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:5389\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[1;32m   5387\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[1;32m   5388\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key)\n\u001b[0;32m-> 5389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m getitem(key)\n\u001b[1;32m   5391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   5392\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[1;32m   5393\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[1;32m   5394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Cell 4: Missing Values Check & Imputation Demo \n",
    "missing_summary = sheet2.isna().sum().sort_values(ascending=False)\n",
    "print(\"=== Missing Value Summary ===\")\n",
    "print(missing_summary[missing_summary > 0] if missing_summary.sum() > 0 else \"✅ No missing values detected.\")\n",
    "\n",
    "# Demo: introduce NaNs into ASTV (or first numeric column), then impute\n",
    "demo_feature = 'ASTV' if 'ASTV' in sheet2.columns else sheet2.select_dtypes(float).columns[0]\n",
    "demo_df = sheet2[[demo_feature]].copy()\n",
    "rng = np.random.default_rng(42)\n",
    "mask_indices = rng.choice(demo_df.index, size=5, replace=False)\n",
    "demo_df.loc[mask_indices, demo_feature] = np.nan\n",
    "\n",
    "before_impute = demo_df.loc[mask_indices]\n",
    "demo_df[demo_feature] = demo_df[demo_feature].fillna(demo_df[demo_feature].median())\n",
    "after_impute = demo_df.loc[mask_indices]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Index': mask_indices,\n",
    "    'Before': before_impute.squeeze().values,\n",
    "    'After': after_impute.squeeze().values\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515f390d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 5: Outlier Detection (Boxplot + Z-Scores)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m available_features \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphysiological\u001b[39m\u001b[38;5;124m'\u001b[39m: sheet2\u001b[38;5;241m.\u001b[39mselect_dtypes(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()}\n\u001b[0;32m----> 3\u001b[0m outlier_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDP\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDP\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sheet2\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m available_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphysiological\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m      6\u001b[0m sns\u001b[38;5;241m.\u001b[39mboxplot(x\u001b[38;5;241m=\u001b[39msheet2[outlier_feature], ax\u001b[38;5;241m=\u001b[39max)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Cell 5: Outlier Detection (Boxplot + Z-Scores)\n",
    "available_features = {'physiological': sheet2.select_dtypes(float).columns.tolist()}\n",
    "outlier_feature = 'DP' if 'DP' in sheet2.columns else available_features['physiological'][0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.boxplot(x=sheet2[outlier_feature], ax=ax)\n",
    "ax.set_title(f'Outlier Check: {outlier_feature}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "dp_scores = stats.zscore(sheet2[outlier_feature].dropna())\n",
    "outlier_mask = np.abs(dp_scores) > 3\n",
    "outlier_indices = sheet2[outlier_feature].dropna().index[outlier_mask]\n",
    "\n",
    "print(\"Outlier samples:\")\n",
    "display(sheet2.loc[outlier_indices, [outlier_feature]].head())\n",
    "print(\"Value counts:\")\n",
    "print(sheet2[outlier_feature].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fea8a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows found: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Duplicate Rows Check\n",
    "duplicate_rows = sheet2.duplicated().sum()\n",
    "print(f\"Duplicate rows found: {duplicate_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a04d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping label leakage columns: []\n",
      "Shape after dropping leakage columns: (2128, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName,Date,SegFile,b,e,LBE,LB,AC,FM,UC,ASTV,MSTV,ALTV,MLTV,DL,DS,DP,DR,Width,Min,Max,Nmax,Nzeros,Mode,Mean,Median,Variance,Tendency,NSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Variab10.txt,1996-12-01,CTG0001.txt,240.0,357....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0002.txt,5.0,632.0,13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0003.txt,177.0,779.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0004.txt,411.0,1192.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fmcs_1.txt,1996-05-03,CTG0005.txt,533.0,1147.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FileName,Date,SegFile,b,e,LBE,LB,AC,FM,UC,ASTV,MSTV,ALTV,MLTV,DL,DS,DP,DR,Width,Min,Max,Nmax,Nzeros,Mode,Mean,Median,Variance,Tendency,NSP\n",
       "0  Variab10.txt,1996-12-01,CTG0001.txt,240.0,357....                                                                                        \n",
       "1  Fmcs_1.txt,1996-05-03,CTG0002.txt,5.0,632.0,13...                                                                                        \n",
       "2  Fmcs_1.txt,1996-05-03,CTG0003.txt,177.0,779.0,...                                                                                        \n",
       "3  Fmcs_1.txt,1996-05-03,CTG0004.txt,411.0,1192.0...                                                                                        \n",
       "4  Fmcs_1.txt,1996-05-03,CTG0005.txt,533.0,1147.0...                                                                                        "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cell 7: Drop Label Leakage Columns (A-SUSP, CLASS, etc.)\n",
    "label_leak_cols = ['CLASS', 'A', 'B', 'C', 'D', 'E', 'AD', 'DE', 'LD', 'FS', 'SUSP']\n",
    "columns_to_drop = [col for col in label_leak_cols if col in sheet2.columns]\n",
    "\n",
    "print(\"Dropping label leakage columns:\", columns_to_drop)\n",
    "clean_df = sheet2.drop(columns=columns_to_drop, errors='ignore')\n",
    "print(\"Shape after dropping leakage columns:\", clean_df.shape)\n",
    "clean_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6930c748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved ctg_clean.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m num_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNSP\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLASS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     10\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 11\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(clean_df[feature_cols])\n\u001b[1;32m     13\u001b[0m df_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_scaled, columns\u001b[38;5;241m=\u001b[39mfeature_cols, index\u001b[38;5;241m=\u001b[39mclean_df\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m clean_df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:914\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m \n\u001b[1;32m    884\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 914\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    915\u001b[0m     X,\n\u001b[1;32m    916\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    917\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[1;32m    918\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    919\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[1;32m    920\u001b[0m )\n\u001b[1;32m    921\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:887\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    883\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    884\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[1;32m    885\u001b[0m )\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m--> 887\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdtypes_orig)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[1;32m    890\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save Cleaned and Scaled Datasets\n",
    "# Save unscaled\n",
    "clean_df.to_csv(\"ctg_clean.csv\", index=False)\n",
    "print(\" Saved ctg_clean.csv\")\n",
    "\n",
    "# Scale numeric features\n",
    "num_cols = clean_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in num_cols if c not in [\"NSP\",\"CLASS\",\"label\"]]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(clean_df[feature_cols])\n",
    "\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=clean_df.index)\n",
    "for col in clean_df.columns:\n",
    "    if col not in feature_cols:\n",
    "        df_scaled[col] = clean_df[col]\n",
    "\n",
    "df_scaled.to_csv(\"ctg_clean_scaled.csv\", index=False)\n",
    "print(\"Saved ctg_clean_scaled.csv\")\n",
    "df_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcb83cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning Summary ===\n",
      "Rows, Cols: (2128, 1)\n",
      "Remaining missing values: 0\n",
      " Target column not found.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Final Cleaning Summary with Charts\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=== Cleaning Summary ===\")\n",
    "print(\"Rows, Cols:\", clean_df.shape)\n",
    "\n",
    "# 1️ Missing values check\n",
    "missing_after = clean_df.isna().sum().sum()\n",
    "print(\"Remaining missing values:\", missing_after)\n",
    "\n",
    "# 2️ Class distribution\n",
    "target_col = \"NSP\" if \"NSP\" in clean_df.columns else (\"CLASS\" if \"CLASS\" in clean_df.columns else None)\n",
    "if target_col:\n",
    "    counts = clean_df[target_col].value_counts().sort_index()\n",
    "    props = counts / counts.sum()\n",
    "\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(counts)\n",
    "    print(\"\\nClass proportions (%):\")\n",
    "    print((props * 100).round(2))\n",
    "\n",
    "    # === Prepare tidy data for seaborn ===\n",
    "    counts_df = counts.reset_index()\n",
    "    counts_df.columns = [\"Class\", \"Count\"]\n",
    "\n",
    "    props_df = props.reset_index()\n",
    "    props_df.columns = [\"Class\", \"Proportion\"]\n",
    "\n",
    "    # === Plot class distribution ===\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Counts barplot\n",
    "    sns.barplot(data=counts_df, x=\"Class\", y=\"Count\", hue=\"Class\", ax=ax[0], palette=\"viridis\", legend=False)\n",
    "    ax[0].set_title(\"Class Distribution (Counts)\")\n",
    "    for i, v in enumerate(counts_df[\"Count\"]):\n",
    "        ax[0].text(i, v + 5, str(v), ha='center')\n",
    "\n",
    "    # Proportions barplot\n",
    "    sns.barplot(data=props_df, x=\"Class\", y=\"Proportion\", hue=\"Class\", ax=ax[1], palette=\"viridis\", legend=False)\n",
    "    ax[1].set_title(\"Class Distribution (Proportions)\")\n",
    "    ax[1].set_ylim(0, 1)\n",
    "    for i, v in enumerate(props_df[\"Proportion\"]):\n",
    "        ax[1].text(i, v + 0.01, f\"{v:.1%}\", ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\" Target column not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52443a2-0429-4451-804d-67e095cc1694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/anaconda3/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/lib/python3.12/site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4568e1bf-ccd8-48fb-83ff-194120227290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned dataset saved as ctg_clean.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Convert cleaned dataset to Excel\n",
    "clean_df.to_excel(\"ctg_clean.xlsx\", index=False)\n",
    "print(\" Cleaned dataset saved as ctg_clean.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9bc7288-07cd-4066-a35d-d2fd873ef7d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#scaled version\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_scaled\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctg_clean_scaled.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Scaled dataset saved as ctg_clean_scaled.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "#scaled version\n",
    "\n",
    "df_scaled.to_excel(\"ctg_clean_scaled.xlsx\", index=False)\n",
    "print(\" Scaled dataset saved as ctg_clean_scaled.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c3f5c-a225-4a5b-be94-b38a3118ecee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
